# used to do pre-training by freezing gemma weights.
lr: 6.e-4 # 2.e-5 #0.00025
lr_decay_fn: "cosine"
lr_end_value: 6.e-9 # used for linear decay only
train_steps: 100000 # 10000 # 800000 # 600000*grad_accum
# warmup_pc: # 0.025 # rought 15000 for 
warmup: 2000
batch_size: 4 # 32 # 60 #  12 #
shuffle_train: True
grad_accumulation_steps: 1
mixed_precision: "bf16" # "no" #
#epochs: 100
weight_decay: 0.01
dataset_name: "tiny-stories"
unroll: 1024
max_seq_len: 4096 #
pos_embed_max_len: 4096 # 32000 #  1024 # 2048
eval_samples: 100 # 40 # average over 30 batches
eval_steps: 100 # 2000 #2000 *grad_accum
max_checkpoints: 2
project: "gemma-mach" # "seqlen-gemma-mach" #  "latte-sota-dev" # 
entity: "baesian-learning"
#check_path: ""
#run_id: "gtbb6lv6"
wandb_log: False #  True #  
disable_cache:  False # 

# dropout each layer
dropout: 0.0
dropout_att: 0.0
prenorm: True
batchnorm: False
hidden_dim: 768 #  512 # 16 # 32 # 512 # 1024 #
intermediate_dim: 2048
nlayers: 12 #12 # 6 # 8
nheads: 8 # 2