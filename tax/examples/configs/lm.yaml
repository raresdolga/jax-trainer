lr: 6.e-4
lr_decay_fn: "cosine"
lr_end_value: 6.e-9 # used for linear decay only
train_steps: 100000 
# warmup_pc: # 0.025
warmup: 2000
batch_size: 4 # 32 # 60 #  12 #
shuffle_train: True
grad_accumulation_steps: 1
mixed_precision: "bf16" # "no" #
#epochs: 100
weight_decay: 0.01
dataset_name: "tiny-stories"
unroll: 1024
max_seq_len: 4096 #
pos_embed_max_len: 4096 
eval_samples: 100 
eval_steps: 100 
max_checkpoints: 2
wandb_log: False #  True #  
disable_cache:  False # 
dropout: 0.0
dropout_att: 0.0
prenorm: True
batchnorm: False
hidden_dim: 768 #  512 # 16 # 32 # 512 # 1024 #
intermediate_dim: 2048
nlayers: 12 #12 # 6 # 8
nheads: 8 # 2